{
 "metadata": {
  "name": "",
  "signature": "sha256:5745cc7d03e88742e49ea3f6f4823b943568131fde17f6ed8acce0a1b48a8f8b"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Programming Assignment I: Exploratory Analysis over 2012 FEC Presidential Election Dataset"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Team Details\n",
      "--------------\n",
      "\n",
      "When submitting, fill your team details in this cell. Note that this is a markdown cell.\n",
      "\n",
      "**Student 1 Name and ID:** Sumesh Nellemakkal Balan   UT ID: 1001119408\n",
      "\n",
      "**Student 2 Name and ID:** Sreehari Balakrishna Hegden, UTA ID: 1001109610\n",
      "\n",
      "**Student 3 Name and ID:** Divya Maria Jose , UTA ID : 1000989859"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Assignment Details\n",
      "-------------------\n",
      "\n",
      "In this assignment, you will be learn more about three key steps in a data analytics process. \n",
      "\n",
      "### 1. Data Collection\n",
      "\n",
      "In the first set of tasks, you will put to use the various techniques we learnt in web scraping and use it to scrape three popular websites. \n",
      "\n",
      "### 2. Exploratory Analysis\n",
      "In the second and third set of tasks, you will conduct a guided exploration over 2012 FEC Presidential Election Dataset. You will learn and use some of the most common exploration/aggregation/descriptive operations. This should also help you learn most of the key functionalities in Pandas.\n",
      "\n",
      "### 3. Visualization\n",
      "In the third set of tasks (that is done in conjunction with #2), you will also learn how to use visualization libraries to identify patterns in data that will help in your further data analysis. You will also explore most popular chart types and how to use different libraries and styles to make your visualizations more attractive.\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Python Packages\n",
      "\n",
      "Here are the set of packages that you will use extensively in this assignment. Do NOT import any new packages without confirming with the instructor. The objective is to make you identify and use the various functions in the packages imported below. Besides, to my knowledge, the libraries below do offer a concise way to achieve the homework tasks anyway."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# special IPython command to prepare the notebook for matplotlib\n",
      "%matplotlib inline \n",
      "\n",
      "#Array processing\n",
      "import numpy as np\n",
      "#Data analysis, wrangling and common exploratory operations\n",
      "import pandas as pd\n",
      "from pandas import Series, DataFrame\n",
      "\n",
      "#A sane way to get web data\n",
      "import requests\n",
      "\n",
      "#Packages for web scraping. No need to use both. Feel free to use one of them.\n",
      "#from pattern import web\n",
      "#from BeautifulSoup import BeautifulSoup\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "#For visualization. Matplotlib for basic viz and seaborn for more stylish figures + statistical figures not in MPL.\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "#For some of the date operations\n",
      "import datetime\n",
      "\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Part 1: Web Scraping\n",
      "----------------\n",
      "\n",
      "In the first series of tasks, we will seek to scrape information from diverse websites. To make your job easier, you can assume that you do not need to do any complex webdriver stuff. For each task, you will implement a function that accepts an url and produces output in the format that the question asks.\n",
      "\n",
      "NOTE: Make sure that you follow the input and output requirements. The assignment will be evaluated and graded automatically by a script. Any error will result in you getting a score of 0 for that task.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "(5 points) Website 1: Wikipedia\n",
      "-----------------------\n",
      "\n",
      "In the first task, we will seek to parse contents from a table in a Wikipedia page. You will implement a function that accepts two parameters. The first is the url and the second is the name of the table. For example, if the url was http://en.wikipedia.org/wiki/List_of_Test_cricket_records and the table name was \"Team wins, losses and draws\", then you have to parse the first table. Your code must produce as an output a Pandas data frame where the columns have same name as the table ('Team',\t'First Test match',\t'Matches',\t'Won',\t'Lost',\t'Tied',\t'Drawn' and '% Won') in this example. \n",
      "\n",
      "Do NOT worry about the data format of the Pandas data frame. Simply treat each of them as a string.\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Input:\n",
      "#    url: URL of a wikipedia page\n",
      "#    table_name: Name of the table to scrape\n",
      "#Output:\n",
      "#    df is a Pandas data frame that contains a tabular representation of the table\n",
      "#    The columns are named the same as the table columns\n",
      "#    Each row of df corresponds to a row in the table\n",
      "#def scraping_wikipedia_table(url, table_name):\n",
      " #   return None\n",
      "\n",
      "from bs4 import BeautifulSoup\n",
      "import urllib2\n",
      "from lxml.html import fromstring\n",
      "import re\n",
      "import csv\n",
      "import pandas as pd\n",
      "#from collections import defaultdic\n",
      "\n",
      "\n",
      "#def scraping_wikipedia_table(parse_url,table_id) :\n",
      "\n",
      "wiki = \"http://en.wikipedia.org/wiki/List_of_Test_cricket_records\"\n",
      "header = {'User-Agent': 'Mozilla/5.0'}\n",
      " #Needed to prevent 403 error on Wikipedia\n",
      "req = urllib2.Request(wiki,headers=header)\n",
      "page = urllib2.urlopen(req)\n",
      "soup = BeautifulSoup(page)\n",
      "\n",
      "#data=defaultdict(list)\n",
      "# Get table\n",
      "try:\n",
      "    table = soup.find_all('table')[1]\n",
      "except AttributeError as e:\n",
      "    print 'No tables found, exiting'\n",
      "   # return 1\n",
      "\n",
      "# Get rows\n",
      "try:\n",
      "    rows = table.find_all('tr')\n",
      "except AttributeError as e:\n",
      "    print 'No table rows found, exiting'\n",
      "    #return 1\n",
      "re1 = []\n",
      "results = [] \n",
      "for row in rows:\n",
      "    table_headers = row.find_all('th')\n",
      "    if table_headers : \n",
      "        re1.append([headers.get_text() for headers in table_headers])\n",
      "    \n",
      "    table_data = row.find_all('td')\n",
      "    if table_data :\n",
      "        results.append([data.get_text() for data in table_data])\n",
      "#print results\n",
      "    # Print data\n",
      "\n",
      "print re1    \n",
      "for i in results:\n",
      "    print '\\t'.join(i)\n",
      "df = pd.DataFrame(data = results)\n",
      "df\n",
      "    # data['Margin'].append(i)  "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[u'Innings and 579 runs'], [u'Innings and 360 runs'], [u'Innings and 336 runs'], [u'Innings and 332 runs'], [u'Innings and 324 runs']]\n",
        "Margin\tTeams\tVenue\tSeason\n",
        "\u00a0England (903-7 d) beat Australia (201 & 123)\tThe Oval, London\t1938\n",
        "\u00a0Australia (652\u20137 d) beat South Africa (159 & 133)\tNew Wanderers Stadium, Johannesburg\t2001\u201302\n",
        "\u00a0West Indies (614\u20135 d) beat India (124 & 154)\tEden Gardens, Kolkata\t1958\u201359\n",
        "\u00a0Australia (645) beat England (141 & 172)\tBrisbane Cricket Ground\t1946\u201347\n",
        "\u00a0Pakistan (643) beat New Zealand (73 & 246)\tGaddafi Stadium, Lahore\t2002\n",
        "\n",
        "Last updated: 10 January 2015[25]\n",
        "\n"
       ]
      },
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>0</th>\n",
        "      <th>1</th>\n",
        "      <th>2</th>\n",
        "      <th>3</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td>                                            Margin</td>\n",
        "      <td>                               Teams</td>\n",
        "      <td>   Venue</td>\n",
        "      <td> Season</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td>     \u00a0England (903-7 d) beat Australia (201 &amp; 123)</td>\n",
        "      <td>                    The Oval, London</td>\n",
        "      <td>    1938</td>\n",
        "      <td>   None</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td> \u00a0Australia (652\u20137 d) beat South Africa (159 &amp; ...</td>\n",
        "      <td> New Wanderers Stadium, Johannesburg</td>\n",
        "      <td> 2001\u201302</td>\n",
        "      <td>   None</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td>     \u00a0West Indies (614\u20135 d) beat India (124 &amp; 154)</td>\n",
        "      <td>               Eden Gardens, Kolkata</td>\n",
        "      <td> 1958\u201359</td>\n",
        "      <td>   None</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td>         \u00a0Australia (645) beat England (141 &amp; 172)</td>\n",
        "      <td>             Brisbane Cricket Ground</td>\n",
        "      <td> 1946\u201347</td>\n",
        "      <td>   None</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>5</th>\n",
        "      <td>       \u00a0Pakistan (643) beat New Zealand (73 &amp; 246)</td>\n",
        "      <td>             Gaddafi Stadium, Lahore</td>\n",
        "      <td>    2002</td>\n",
        "      <td>   None</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>6</th>\n",
        "      <td>             \\nLast updated: 10 January 2015[25]\\n</td>\n",
        "      <td>                                None</td>\n",
        "      <td>    None</td>\n",
        "      <td>   None</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 1,
       "text": [
        "                                                   0  \\\n",
        "0                                             Margin   \n",
        "1      \u00a0England (903-7 d) beat Australia (201 & 123)   \n",
        "2  \u00a0Australia (652\u20137 d) beat South Africa (159 & ...   \n",
        "3      \u00a0West Indies (614\u20135 d) beat India (124 & 154)   \n",
        "4          \u00a0Australia (645) beat England (141 & 172)   \n",
        "5        \u00a0Pakistan (643) beat New Zealand (73 & 246)   \n",
        "6              \\nLast updated: 10 January 2015[25]\\n   \n",
        "\n",
        "                                     1        2       3  \n",
        "0                                Teams    Venue  Season  \n",
        "1                     The Oval, London     1938    None  \n",
        "2  New Wanderers Stadium, Johannesburg  2001\u201302    None  \n",
        "3                Eden Gardens, Kolkata  1958\u201359    None  \n",
        "4              Brisbane Cricket Ground  1946\u201347    None  \n",
        "5              Gaddafi Stadium, Lahore     2002    None  \n",
        "6                                 None     None    None  "
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "### (5 points)  Website 2: Walmart\n",
      "\n",
      "In the second task, we will seek to scrape the results of Walmart. Specifically, we will focus on Movies department. Your job is to implement a function that accepts a valid search result url  (such as http://www.walmart.com/search/?query=marvel&cat_id=4096_530598 ), scrapes the 10-15 product listings in that url only and outputs a Pandas data frame with the following fields (with the EXACT names and data types as below): \n",
      "\n",
      "+ Product Title: String. Title of the product (such as Samsung Red WB1100F Smart Camera with 16.2 Megapixels and 35x Optical Zoom)\n",
      "+ Sale Price: Float. Price of the product (such as 249.99). This is the price that is highlighted in blue typically. Ignore the dollar sign and return the value only\n",
      "+ Number of Ratings: Integer. The number of ratings that users have provided for the product. This is the number associated with the star.\n",
      "+ Free Shipping: Boolean. Is True if the product has free shipping above $50 and false otherwise. \n",
      "+ Free Store Pickup: Boolean. Is True if the product has free store pickup and false otherwise. \n",
      "+ Starring: String. Contains the name of starring actors. Convert it to a simple string such as \"Chris Pratt Zoe Saldana Vin Diesel\" is okay. No need to parse it further.\n",
      "+ Running: Integer. The running time of the movie. Only the integer value is required (ie. NNNN minutes then return only NNNN).\n",
      "+ Format: String. Values such as Widescreen. Beware, some entries might not have this value.\n",
      "+ Rating: String. MPAA rating.\n",
      "\n",
      "Make sure that you observe the following:\n",
      "\n",
      "* If a data type is specified, then your data should be in that format. For eg, if a field is int, then ensure that it has an integer value.\n",
      "* Your code should not crash if some product did not have a valid value such as price. Instead you must fill it with NA (see Pandas tutorial for what NA is). "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Input:\n",
      "#    url: URL of a Walmart results page for a search query in Movies department\n",
      "#Output:\n",
      "#    df is a Pandas data frame that contains a tabular representation of the results\n",
      "#    The df must have 9 columns that must have the same name and data type as described above\n",
      "#    Each row of df corresponds to a movie in the results table\n",
      "#def scraping_walmart_movies(url):\n",
      " #   return None\n",
      "    \n",
      "from collections import defaultdict\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "import csv\n",
      "import pandas as pd\n",
      "\n",
      "\n",
      "\n",
      "r= requests.get(\"http://www.walmart.com/search/?query=marvel&cat_id=4096_530598\")\n",
      "r.content\n",
      "\n",
      "soup = BeautifulSoup(r.content)\n",
      "\n",
      "g_data = soup.find_all(\"div\", {\"class\" : \"tile-content\"})\n",
      "g_price = soup.find_all(\"div\",{\"class\" : \"item-price-container\"})\n",
      "g_star = soup.find_all(\"div\",{\"class\" : \"stars stars-small tile-row\"})\n",
      "\n",
      "#all the title inside tile-content\n",
      "data=defaultdict(list)\n",
      "for product_title in g_data:\n",
      "    a_product_title = product_title.find_all(\"a\",\"js-product-title\")\n",
      "    for text_product_title in a_product_title : \n",
      "       data['Product Title'].append(text_product_title.text)  \n",
      "\n",
      "for row in g_price:\n",
      "     price = row.find('span', 'price price-display').text.strip()\n",
      "     data['Price'].append(price)\n",
      "\n",
      "        \n",
      "for allstar in g_star:\n",
      "    star=allstar.find('span','visuallyhidden').text.strip()\n",
      "    print star\n",
      "    #data['Star'].append(star)\n",
      "\n",
      "     #price['Price'].append(price.text)\n",
      "\n",
      "df = pd.DataFrame(data)\n",
      "df\n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "4.5 stars\n",
        "5.0 stars\n",
        "5.0 stars\n",
        "4.0 stars\n",
        "5.0 stars\n",
        "4.5 stars\n",
        "5.0 stars\n",
        "4.5 stars\n",
        "5.0 stars\n",
        "4.5 stars\n",
        "5.0 stars\n",
        "4.5 stars\n",
        "5.0 stars\n",
        "4.5 stars\n",
        "5.0 stars\n"
       ]
      },
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>Price</th>\n",
        "      <th>Product Title</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0 </th>\n",
        "      <td> $14.96</td>\n",
        "      <td>      Marvel: Guardians Of The Galaxy (Widescreen)</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1 </th>\n",
        "      <td> $33.98</td>\n",
        "      <td>                         Marvel Heroes: Collection</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2 </th>\n",
        "      <td> $26.96</td>\n",
        "      <td>              Marvel Complete Giftset (Widescreen)</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3 </th>\n",
        "      <td> $19.96</td>\n",
        "      <td>                Marvel's The Avengers (Widescreen)</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4 </th>\n",
        "      <td> $13.47</td>\n",
        "      <td> Marvel Knights: Wolverine Versus Sabretooth - ...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>5 </th>\n",
        "      <td> $13.47</td>\n",
        "      <td> Superheroes Collection: The Incredible Hulk Re...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>6 </th>\n",
        "      <td> $19.96</td>\n",
        "      <td> Marvel: Iron Man &amp; Hulk - Heroes United (Wides...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>7 </th>\n",
        "      <td> $18.75</td>\n",
        "      <td>  Captain America: The Winter Soldier (Widescreen)</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>8 </th>\n",
        "      <td> $19.96</td>\n",
        "      <td>      Iron Man 3 (DVD + Digital Copy) (Widescreen)</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>9 </th>\n",
        "      <td> $29.96</td>\n",
        "      <td> Marvel's The Avengers (DVD + Blu-ray) (Widescr...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>10</th>\n",
        "      <td> $19.96</td>\n",
        "      <td>                 Thor: The Dark World (Widescreen)</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>11</th>\n",
        "      <td> $12.96</td>\n",
        "      <td> Spider-Man (2-Disc) (Special Edition) (Widescr...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>12</th>\n",
        "      <td> $25.46</td>\n",
        "      <td> Spider-Man / Spider-Man 2 / Spider-Man 3 (Wide...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>13</th>\n",
        "      <td> $13.99</td>\n",
        "      <td> Elektra / Fantastic Four / Daredevil (Director...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>14</th>\n",
        "      <td> $12.96</td>\n",
        "      <td>                         Spider-Man 2 (Widescreen)</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>15</th>\n",
        "      <td>  $6.99</td>\n",
        "      <td>          The Punisher (Extended Cut) (Widescreen)</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>16</th>\n",
        "      <td>  $9.07</td>\n",
        "      <td> DC Showcase: Superman / Shazam!: The Return Of...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>17</th>\n",
        "      <td> $11.17</td>\n",
        "      <td>   Ultimate Avengers Movie Collection (Widescreen)</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>18</th>\n",
        "      <td>  $5.25</td>\n",
        "      <td>         Ultimate Avengers: The Movie (Widescreen)</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>19</th>\n",
        "      <td>  $5.00</td>\n",
        "      <td> The Next Avengers: Heroes Of Tomorrow (Widescr...</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "     Price                                      Product Title\n",
        "0   $14.96       Marvel: Guardians Of The Galaxy (Widescreen)\n",
        "1   $33.98                          Marvel Heroes: Collection\n",
        "2   $26.96               Marvel Complete Giftset (Widescreen)\n",
        "3   $19.96                 Marvel's The Avengers (Widescreen)\n",
        "4   $13.47  Marvel Knights: Wolverine Versus Sabretooth - ...\n",
        "5   $13.47  Superheroes Collection: The Incredible Hulk Re...\n",
        "6   $19.96  Marvel: Iron Man & Hulk - Heroes United (Wides...\n",
        "7   $18.75   Captain America: The Winter Soldier (Widescreen)\n",
        "8   $19.96       Iron Man 3 (DVD + Digital Copy) (Widescreen)\n",
        "9   $29.96  Marvel's The Avengers (DVD + Blu-ray) (Widescr...\n",
        "10  $19.96                  Thor: The Dark World (Widescreen)\n",
        "11  $12.96  Spider-Man (2-Disc) (Special Edition) (Widescr...\n",
        "12  $25.46  Spider-Man / Spider-Man 2 / Spider-Man 3 (Wide...\n",
        "13  $13.99  Elektra / Fantastic Four / Daredevil (Director...\n",
        "14  $12.96                          Spider-Man 2 (Widescreen)\n",
        "15   $6.99           The Punisher (Extended Cut) (Widescreen)\n",
        "16   $9.07  DC Showcase: Superman / Shazam!: The Return Of...\n",
        "17  $11.17    Ultimate Avengers Movie Collection (Widescreen)\n",
        "18   $5.25          Ultimate Avengers: The Movie (Widescreen)\n",
        "19   $5.00  The Next Avengers: Heroes Of Tomorrow (Widescr..."
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### (2.5 * 4 = 10 points) Website 3: Facebook\n",
      "\n",
      "In the third task, we will scrape some specific sub-pages of Facebook profiles. In order to avoid using sophisticated tools like Selenium, for this task you can assume that the input to the function is the DOM of the relevant page. You can then use it to parse relevant contents."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Input: dom - DOM of the books page corresponding to an FB account's profile. Eg, DOM of https://www.facebook.com/zuck/books\n",
      "#Output: An array (Python list) of books listed in the profile page. \n",
      "#    Note that this function requires a list as an output not a Pandas data frame\n",
      "def scraping_facebook_books(dom):\n",
      "    return []\n",
      "\n",
      "#Input: dom - DOM of the groups page corresponding to an FB account's profile. Eg, DOM of https://www.facebook.com/zuck/groups\n",
      "#Output: A Pandas data frame with one row per group. \n",
      "#    It must have three columns - 'Group Name', 'Number of Members', 'Group Description'\n",
      "#    Note that all information of a group is in the same page (eg. https://www.facebook.com/zuck/groups)\n",
      "#    You can collect all data from same page even if they are incomplete (such as group description)\n",
      "#    Ensure that the column names as given above\n",
      "def scraping_facebook_groups(dom):\n",
      "    return None\n",
      "\n",
      "#Input: dom - DOM of the music page corresponding to an FB account's profile. Eg, DOM of https://www.facebook.com/zuck/music\n",
      "#Output: A Pandas data frame with one row per group. \n",
      "#    It must have four columns \n",
      "#    'Name', 'Type' (eg. Musician/Band or Bands&Musicians) and 'Verified' (boolean: True/False), 'Profile Url'\n",
      "#    Note that all information of a group is in the same page (eg. https://www.facebook.com/zuck/music)\n",
      "#    Ensure that the column names as given above\n",
      "def scraping_facebook_music(dom):\n",
      "    return None\n",
      "\n",
      "#Let us now make things little bit more harder. \n",
      "#In all previous cases, you only had to collect information from a single page.\n",
      "# But in reality, you have to collect information and integrate from multiple pages.\n",
      "# Let us try a simple version of such data integration \n",
      "\n",
      "#Input: dom - DOM of the music page corresponding to an FB account's profile. Eg, DOM of https://www.facebook.com/zuck/movies\n",
      "#Output: A Pandas data frame with one row per group. \n",
      "#    It must have following columns - \n",
      "#        'Name', 'Type' (eg. Movie), 'Verified', 'Profile Url' - as before\n",
      "#        'Likes', 'Starring', 'Genre', 'Director', 'Movie URL'\n",
      "\n",
      "#    The first three columns can be obtained from https://www.facebook.com/zuck/movies\n",
      "#    Once you get the profile url, obtain the HTML of this url and use this content to obtain the last 5 column data\n",
      "#    For example, Zuckerberg likes 'The Matrix' (great movie btw). \n",
      "#    Then you get its profile url 'https://www.facebook.com/TheMatrixMovie?ref=profile'\n",
      "#    Get the text of this url using requests package and parse information from the About tab\n",
      "#    Ensure that the column names as given above\n",
      "def scraping_facebook_movies(dom):\n",
      "    return None"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 63
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Part 2 and 3: Exploratory Analysis and Visualization\n",
      "======================\n",
      "\n",
      "Data Set Description\n",
      "---------------------\n",
      "\n",
      "In this assignment, you will use the FEC dataset. The US Federal Election Commission publishes data on contributions to political campaigns. This includes contributor names, occupation and employer, address, and contribution amount. Specifically, we will be using the FEC data from 2012 election between Barack Obama and Mitt Romney. This is widely considered as a landmark election as both sides spent an unprecedented amount of 1 Billion dollars each (i.e. more than 6000 Crores in INR each). If you are interested, you can download the entire list of contributor details at the [FEC site](http://www.fec.gov/disclosurep/PDownload.do). It is relatively large (150 MB compressed, 1 GB uncompressed). For our experiments, we will use a smaller subset of the data collected (and cleaned) by Wes McKinney (the creator of Pandas). For the download link, see the [Assignments page](http://saravanan-thirumuruganathan.github.io/cse5334Spring2015/assignments.html). It is small by most standards (around 1.4 million records, 20 MB compressed, 160 MB uncompressed) but large enough to give a taste of why data mining is compute intensive. Hopefully, this will also give you an appreciation as to the awesomeness of Pandas/Numpy - you can do really cool stuff with 2-3 lines of code that runs in seconds. \n",
      "\n",
      "**Dataset Description:** You can find the details (such as meaning of the column names) of the dataset in the [FEC website](ftp://ftp.fec.gov/FEC/Presidential_Map/2012/DATA_DICTIONARIES/CONTRIBUTOR_FORMAT.txt).\n",
      "\n",
      "While a knowledge of US Elections or FEC Campaign finance rules is not necessary for solving this assignment, you can check out the following links if you are curious. \n",
      "\n",
      "[How to Become the US President: A Step-by-Step Guide](http://2012election.procon.org/view.resource.php?resourceID=004333)\n",
      "\n",
      "[US Election guide: how does the election work?](http://www.telegraph.co.uk/news/worldnews/us-election/9480396/US-Election-guide-how-does-the-election-work.html)\n",
      "\n",
      "If you are fascinated with Campaign finance reform (as I am), here are some good links:\n",
      "\n",
      "[Super-PACs and Dark Money: ProPublica\u2019s Guide to the New World of Campaign Finance](http://www.propublica.org/blog/item/super-pacs-propublicas-guide-to-the-new-world-of-campaign-finance)\n",
      "\n",
      "[40 charts that explain money in politics](http://www.vox.com/2014/7/30/5949581/money-in-politics-charts-explain)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Visualization\n",
      "-------------\n",
      "\n",
      "Visualization is a key component of exploration. You will perform a number of 1-D and 2-D charts to get some intuition about the data. You can choose to use either Matplotlib or Seaborn for plotting. The default figures generated from Matplotlib might look a bit ugly. So you might want to try Seaborn to get better figures. The defaults in Seaborn are much saner and sometimes makes your life lot easier. Seaborn has number of styles - so feel free to experiment with them and choose the one you like. We have earmarked 10 points for the aesthetics of your visualizations.  "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Pandas\n",
      "----------\n",
      "Almost all the tasks below could be purely solved using Pandas. Most analytic tasks should not require more than 2-3 lines of code (visualization on the other hand is a different matter - you might need 5-10 lines for each chart unless you use Seaborn). Here is a NON COMPREHENSIVE list of functions that you might want to know to solve this assignment : agg, apply, argmax, argmin, count, crosstab, cumsum, cut, describe, groupby, head, idxmax, idxmin, info, isin, map, max, min, pivot_table, size, sum, transform, unique, value_counts .\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Exploration Tasks\n",
      "==================\n",
      "\n",
      "You can find a set of exploratory analytics tasks below. Ensure that you clearly follow the instructions. The assignment will be graded automatically - so failure to follow might cause some issues. Also do NOT rename the functions or variables that the instructor has set. Of course, feel free to create new variables/functions that will hep your code. \n",
      "\n",
      "Reading and Filtering Dataset\n",
      "------------------------------\n",
      "The FEC data contains information about all presidential candidates. As a sitting president, Barack Obama was the only candidate from the Democratic party. In the Republican party, there was a process called Primary (see links above) where number of candidates competed to be the nominee. Mitt Romney won the Republican primary and competed with Barack Obama in the elections, which Obama won. \n",
      "\n",
      "The Python code below reads the FEC dataset into a Pandas data frame with the name fec_all. If your machine has less than 2 GB of RAM, then change the function argument low_memory to True. Once the frame is loaded, we remove all negative contributions (where the campaign refunded amount to a contributor for some reason). Finally, we create a new data frame called fec that contains the contributions to Barack Obama and Mitt Romney alone. \n",
      "\n",
      "For this code to work, the file 'fec_2012_contribution_subset.csv' must be in the same folder as the notebook. \n",
      "\n",
      "To reduce my typing, I might refer to Obama as BO and Romney as MR in the text below."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#read the csv file into a Pandas data frame\n",
      "fec_all = pd.read_csv('fec_2012_contribution_subset.csv', low_memory=False)\n",
      "\n",
      "#We will be doing party wise analysis later. So, we want to associate each candidate with their party\n",
      "parties = {'Bachmann, Michelle': 'Republican',\n",
      "           'Cain, Herman': 'Republican',\n",
      "           'Gingrich, Newt': 'Republican',\n",
      "           'Huntsman, Jon': 'Republican',\n",
      "           'Johnson, Gary Earl': 'Republican',\n",
      "           'McCotter, Thaddeus G': 'Republican',\n",
      "           'Obama, Barack': 'Democrat',\n",
      "           'Paul, Ron': 'Republican',\n",
      "           'Pawlenty, Timothy': 'Republican',\n",
      "           'Perry, Rick': 'Republican',\n",
      "           \"Roemer, Charles E. 'Buddy' III\": 'Republican',\n",
      "           'Romney, Mitt': 'Republican',\n",
      "           'Santorum, Rick': 'Republican'}\n",
      "\n",
      "#create a new column called party that sets the value to the party of the candidate\n",
      "# The way this line works is as follows:\n",
      "#  1. fec_all.cand_nm gives a vector (or Series in Pandas terminology)\n",
      "#  2. For each row, the code looks up the candidate name to the dictionary parties\n",
      "#  3. If the name of the candidate (cand_nm) is in parties, it returns the value (i.e. Republican or Democrat)\n",
      "#  4. This whole thing is done for each row and you get another vector as output\n",
      "#  5. Finally, you create a new column in fec_all called 'party' and assign the vector you got to it.\n",
      "#  6. All in a single line :)\n",
      "fec_all['party'] = fec_all.cand_nm.map(parties)\n",
      "\n",
      "#ignore the refunds\n",
      "# Get the subset of dataset where contribution amount is positive\n",
      "fec_all = fec_all[fec_all.contb_receipt_amt > 0]\n",
      "\n",
      "#fec_all contains details about all presidential candidates. \n",
      "#fec contains the details about contributions to Barack Obama and Mitt Romney only\n",
      "#for the rest of the tasks, unless explicitly specified, work on the fec data frame.\n",
      "fec = fec_all[fec_all.cand_nm.isin(['Obama, Barack', 'Romney, Mitt'])]\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Task 1: Descriptive Statistics\n",
      "--------------------------------\n",
      "\n",
      "Let us start with some easy stuff. As of now, you do not know anything about the dataset. So the first task will be to get some basic sense. \n",
      "\n",
      " "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Task 1a: print the details of the data frame. \n",
      "# Basically, this should contain information such as number of rows,columns, name of columns, #non-null values for each column etc\n",
      "# Hint: there is a Pandas function to do this\n",
      "#replace None with your code\n",
      "print \"Task 1a: Details of FEC data frame are: \\n\", fec.info(verbose=True)\n",
      "\n",
      "#Task 1b: finding the number of rows and columns in the data frame.\n",
      "#Hint: find the property of the data frame that gives the number of rows and columns\n",
      "#replace None with your code\n",
      "t1b_num_rows = fec.shape[0]\n",
      "t1b_num_cols = fec.shape[1]\n",
      "print \"Task 1b: #Rows=%s, #Columns=%s\" % (t1b_num_rows, t1b_num_cols) \n",
      "\n",
      "#Task 1c: The only numeric data is 'contb_receipt_amt' which is the amount of contribution. \n",
      "# Print the descriptive details (min, max, quantiles etc) for 'contb_receipt_amt'\n",
      "#Hint: as above there is a direct pandas command for it.\n",
      "#replace None with your code\n",
      "print \"Task 1c: Descriptive details of contb_receipt_amt is \\n\", fec[\"contb_receipt_amt\"].describe()\n",
      "\n",
      "#Task 1d: Let us now print the number of unique values for few columns\n",
      "#Hint: Look for a Pandas function to do this.\n",
      "t1d_num_uniq_cand_id = fec[\"cand_id\"].unique()\n",
      "t1d_num_uniq_cand_nm = fec[\"cand_nm\"].unique()\n",
      "t1d_num_uniq_contbr_city = fec[\"contbr_city\"].unique()\n",
      "t1d_num_uniq_contbr_st = fec[\"contbr_st\"].unique()\n",
      "print \"Task 1d: #Uniq cand_id = \", len(t1d_num_uniq_cand_id)\n",
      "print \"Task 1d: #Uniq cand_nm = \", len(t1d_num_uniq_cand_nm)\n",
      "print \"Task 1d: #Uniq contbr_city = \", len(t1d_num_uniq_contbr_city)\n",
      "print \"Task 1d: #Uniq contbr_st = \", len(t1d_num_uniq_contbr_st)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Task 1a: Details of FEC data frame are: \n",
        "<class 'pandas.core.frame.DataFrame'>\n",
        "Int64Index: 694282 entries, 411 to 701385\n",
        "Data columns (total 17 columns):\n",
        "cmte_id              694282 non-null object\n",
        "cand_id              694282 non-null object\n",
        "cand_nm              694282 non-null object\n",
        "contbr_nm            694282 non-null object\n",
        "contbr_city          694275 non-null object\n",
        "contbr_st            694278 non-null object\n",
        "contbr_zip           694234 non-null object\n",
        "contbr_employer      693607 non-null object\n",
        "contbr_occupation    693524 non-null object\n",
        "contb_receipt_amt    694282 non-null float64\n",
        "contb_receipt_dt     694282 non-null object\n",
        "receipt_desc         2345 non-null object\n",
        "memo_cd              87387 non-null object\n",
        "memo_text            90672 non-null object\n",
        "form_tp              694282 non-null object\n",
        "file_num             694282 non-null int64\n",
        "party                694282 non-null object\n",
        "dtypes: float64(1), int64(1), object(15)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "None\n",
        "Task 1b: #Rows=694282, #Columns=17\n",
        "Task 1c: Descriptive details of contb_receipt_amt is \n",
        "count     694282.000000\n",
        "mean         322.942745\n",
        "std         4482.130250\n",
        "min            0.010000\n",
        "25%           35.000000\n",
        "50%          100.000000\n",
        "75%          250.000000\n",
        "max      2014490.510000\n",
        "dtype: float64"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Task 1d: #Uniq cand_id = "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2\n",
        "Task 1d: #Uniq cand_nm =  2\n",
        "Task 1d: #Uniq contbr_city =  11857\n",
        "Task 1d: #Uniq contbr_st =  68\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Task 2: Basic Filtering\n",
      "-----------------------\n",
      "In this task, we will perform some very high level filtering. Pandas has a convenient and powerful syntax for filtering (for eg, see above for how I filtered negative contributions and non-Obama, Romney candidates). "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Task 2a: Let us find out how much contributions did Obama and Romney made in this dataset\n",
      "# Remember, that this is not the complete amount as it excludes other sources like PACs, Super PACs and \n",
      "#  spending by party committes.\n",
      "#Hint: use cand_nm field\n",
      "t2a_tot_amt_obama = len(fec[fec.cand_nm.isin(['Obama, Barack'])])\n",
      "t2a_tot_amt_romney = len(fec[fec.cand_nm.isin(['Romney, Mitt'])])\n",
      "print \"Task 2a: Total Contribution for Obama is %s and for Romney is %s\" % (t2a_tot_amt_obama, t2a_tot_amt_romney)\n",
      "\n",
      "#Task 2b: How much contribution did folks from California, New York and Texas make totally (i.e. to both Obama and Romney).\n",
      "#use contbr_st field\n",
      "t2b_tot_amt_CA = len(fec[fec.contbr_st.isin(['CA'])])\n",
      "t2b_tot_amt_NY = len(fec[fec.contbr_st.isin(['NY'])])\n",
      "t2b_tot_amt_TX = len(fec[fec.contbr_st.isin(['TX'])])\n",
      "print \"Task 2b: Total contributions from California is %s, New York is %s and Texas is %s\" % (t2b_tot_amt_CA, t2b_tot_amt_NY, t2b_tot_amt_TX)\n",
      "\n",
      "#Task 2c: Let us now use multiple filtering criteria\n",
      "# How much money did folks from Texas made to BO and MR?\n",
      "# How much money did folks from UT Arlington made to BO and MR?\n",
      "\n",
      "temp_t2c_tot_contr_tx_bo = fec[fec.cand_nm.isin(['Obama, Barack']) & fec.contbr_st.isin(['TX'])]\n",
      "t2c_tot_contr_tx_bo = temp_t2c_tot_contr_tx_bo['contb_receipt_amt'].sum()\n",
      "\n",
      "temp_t2c_tot_contr_tx_mr = fec[fec.cand_nm.isin(['Romney, Mitt']) & fec.contbr_st.isin(['TX'])]\n",
      "t2c_tot_contr_tx_mr = temp_t2c_tot_contr_tx_mr['contb_receipt_amt'].sum()\n",
      "\n",
      "temp_t2c_tot_contr_uta_bo = fec[fec.cand_nm.isin(['Obama, Barack']) & fec.contbr_employer.isin(['UT ARLINGTON'])]\n",
      "t2c_tot_contr_uta_bo = temp_t2c_tot_contr_uta_bo['contb_receipt_amt'].sum()\n",
      "\n",
      "temp_t2c_tot_contr_uta_mr = fec[fec.cand_nm.isin(['Romney, Mitt']) & fec.contbr_employer.isin(['UT ARLINGTON'])]\n",
      "t2c_tot_contr_uta_mr = temp_t2c_tot_contr_uta_mr['contb_receipt_amt'].sum()\n",
      "\n",
      "print \"Task 2c: From TX, BO got %s and MR got %s dollars\" % (t2c_tot_contr_tx_bo, t2c_tot_contr_tx_mr)\n",
      "print \"Task 2c: From UTA, BO got %s and MR got %s dollars\" % (t2c_tot_contr_uta_bo, t2c_tot_contr_uta_mr)\n",
      "\n",
      "#Task 2d: How much did Engineers from Google gave to BO and MR.\n",
      "# This task is a bit tricky as there are many variations: eg, SOFTWARE ENGINEER vs ENGINEER and GOOGLE INC. vs GOOGLE\n",
      "temp_t2d_tot_engr_goog_bo = fec[fec.cand_nm.isin(['Obama, Barack']) & fec.contbr_employer.str.contains('GOOGLE') & fec.contbr_occupation.str.contains('ENG')]\n",
      "t2d_tot_engr_goog_bo = temp_t2d_tot_engr_goog_bo['contb_receipt_amt'].sum()\n",
      "\n",
      "temp_t2d_tot_engr_goog_mr = fec[fec.cand_nm.isin(['Romney, Mitt']) & fec.contbr_employer.str.contains('GOOGLE') & fec.contbr_occupation.str.contains('ENG')]\n",
      "t2d_tot_engr_goog_mr = temp_t2d_tot_engr_goog_mr['contb_receipt_amt'].sum()\n",
      "\n",
      "print \"Task 2d: From Google Engineers, BO got %s and MR got %s dollars\" % (t2d_tot_engr_goog_bo, t2d_tot_engr_goog_mr)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Task 2a: Total Contribution for Obama is 589127 and for Romney is 105155\n",
        "Task 2b: Total contributions from California is 113539, New York is 58346 and Texas is 39114"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Task 2c: From TX, BO got 6570832.45 and MR got 6221989.68 dollars"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Task 2c: From UTA, BO got 750.0 and MR got 0 dollars\n",
        "Task 2d: From Google Engineers, BO got 88312.4 and MR got 2850.0 dollars"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 76
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Task 3: Basic Aggregation \n",
      "--------------------------\n",
      "In this task, we will perform some very high level aggregation. Pandas has some convenient functions for aggregation (do NOT write a for loop - Pandas has some very efficient, vectorized code)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Task 3a: For each state, print the total contribution they made to both candidates. \n",
      "grouped_t3a_state_contr_both = fec.groupby(['contbr_st']).sum()\n",
      "t3a_state_contr_both = grouped_t3a_state_contr_both['contb_receipt_amt']\n",
      "print \"Task 3a: Total contribution made to both candidates by each state are:\\n\", t3a_state_contr_both\n",
      "\n",
      "#Task 3b: Now let us limit ourselves to TX. For each city in TX, print the total contribution made to both candidates\n",
      "temp_t3b_tx_city_contr_both = fec[fec.contbr_st.isin(['TX'])]\n",
      "grouped_t3b_tx_city_contr_both = temp_t3b_tx_city_contr_both.groupby(['contbr_city']).sum()\n",
      "t3b_tx_city_contr_both = grouped_t3b_tx_city_contr_both['contb_receipt_amt']\n",
      "print \"Task 3b: Total contribution made to both candidates by each city in TX are:\\n\", t3b_tx_city_contr_both\n",
      "\n",
      "#Task 3c: Now let us zoom into  Arlington, TX. For each zipcode in Arlington, print the total contribution made to both candidates\n",
      "temp_t3c_arlington_contr_both = fec[fec.contbr_st.isin(['TX']) & fec.contbr_city.isin(['ARLINGTON'])]\n",
      "grouped_t3c_arlington_contr_both = temp_t3c_arlington_contr_both.groupby(['contbr_zip']).sum()\n",
      "t3c_arlington_contr_both = grouped_t3c_arlington_contr_both['contb_receipt_amt']\n",
      "print \"Task 3c: Total contribution made to both candidates by each zipcode in Arlington are:\\n\", t3c_arlington_contr_both"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Task 3a: Total contribution made to both candidates by each state are:\n",
        "contbr_st\n",
        "AA              56540.00\n",
        "AB               2048.00\n",
        "AE              48653.75\n",
        "AK             368044.39\n",
        "AL            1070426.99\n",
        "AP              38785.50\n",
        "AR             464803.28\n",
        "AS               2955.00\n",
        "AZ            3394913.21\n",
        "CA           35062620.84\n",
        "CO            3639143.61\n",
        "CT            5567766.71\n",
        "DC            5398676.30\n",
        "DE             419381.14\n",
        "FF              99030.00\n",
        "...\n",
        "SC            1033475.80\n",
        "SD             253088.76\n",
        "TN            2636233.06\n",
        "TX           12792822.13\n",
        "UK               2500.00\n",
        "UT            4237151.85\n",
        "VA            7725743.04\n",
        "VI              84212.00\n",
        "VT            1041740.03\n",
        "WA            5592454.72\n",
        "WI            1400471.78\n",
        "WV             295879.59\n",
        "WY             446642.58\n",
        "XX             400250.00\n",
        "ZZ               5963.00\n",
        "Name: contb_receipt_amt, Length: 67, dtype: float64\n",
        "Task 3b: Total contribution made to both candidates by each city in TX are:\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "contbr_city\n",
        "ABERNATHY          210.00\n",
        "ABILENE          45748.00\n",
        "ADDISON          12244.25\n",
        "ADKINS             600.00\n",
        "ALAMO              152.00\n",
        "ALAMO HEIGHTS     2500.00\n",
        "ALBA                69.00\n",
        "ALBANY            6500.00\n",
        "ALEDO             5880.00\n",
        "ALICE             2760.00\n",
        "ALLEN            21066.00\n",
        "ALLISON            474.00\n",
        "ALPINE            5166.00\n",
        "ALTO               120.00\n",
        "ALTON              125.00\n",
        "...\n",
        "WIMBERLEY      19060\n",
        "WINDCREST        525\n",
        "WINDTHORST       400\n",
        "WINFIELD        1000\n",
        "WINNIE           230\n",
        "WINNSBORO        375\n",
        "WOODLANDS       5112\n",
        "WOODVILLE        175\n",
        "WOODWAY         2965\n",
        "WORTHAM          250\n",
        "WYLIE           8405\n",
        "YANTIS           340\n",
        "YOAKUM           100\n",
        "YORKTOWN         100\n",
        "ZAPATA           300\n",
        "Name: contb_receipt_amt, Length: 729, dtype: float64\n",
        "Task 3c: Total contribution made to both candidates by each zipcode in Arlington are:\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "contbr_zip\n",
        "75237            85\n",
        "76001          1130\n",
        "760015368      5175\n",
        "760015696       150\n",
        "760017554       300\n",
        "76002           904\n",
        "760023037       550\n",
        "760024202       215\n",
        "760025010      2474\n",
        "760025461        35\n",
        "760041185      1000\n",
        "76006         10050\n",
        "760062026       400\n",
        "760062725      1000\n",
        "760062778       365\n",
        "...\n",
        "760172730      125\n",
        "760172754     1000\n",
        "760172768       80\n",
        "760173004      135\n",
        "760173541      636\n",
        "760174554      200\n",
        "760176032      210\n",
        "760176250      145\n",
        "760177925       25\n",
        "760177973      650\n",
        "76018          105\n",
        "760181913      170\n",
        "760182501      100\n",
        "760184920      100\n",
        "760185108       35\n",
        "Name: contb_receipt_amt, Length: 121, dtype: float64\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Task 4: Aggregation+Filtering+Ranking\n",
      "-----------------------------------------\n",
      "In this task, you will try to combine aggregation with filtering and then rank the results based on the results. Pandas is often quite clever and might sort the data for you already. \n",
      "\n",
      "**Hint:** Pandas has ready made functions for all the following."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Task 4a: Print the number of contributors to Obama in each state.\n",
      "temp_t4a_num_contr_obama_per_state = fec[fec.cand_nm.isin(['Obama, Barack'])]\n",
      "t4a_num_contr_obama_per_state = temp_t4a_num_contr_obama_per_state.groupby(['contbr_st']).contbr_nm.nunique()\n",
      "print \"Number of contributions to Obama in each state is:\\n\", t4a_num_contr_obama_per_state\n",
      "\n",
      "#Task 4b: Print the top-10 states (based on number of contributors) that contributed to Obama.\n",
      "# print both state name and number of contributors\n",
      "temp_t4b_top10_obama_contr_states = fec[fec.cand_nm.isin(['Obama, Barack'])]\n",
      "grouped_t4b_top10_obama_contr_states = temp_t4b_top10_obama_contr_states.groupby('contbr_st').contbr_nm.nunique().order(ascending=False)\n",
      "t4b_top10_obama_contr_states = grouped_t4b_top10_obama_contr_states[0:10]\n",
      "print \"\\nTop-10 states with most contributors to Obama are:\\n\", t4b_top10_obama_contr_states\n",
      "\n",
      "#Task 4c: Print the top-20 occupations that contributed overall (to both BO and MR)\n",
      "grouped_t4c_top20_contr_occupation = fec.groupby('contbr_occupation').contb_receipt_amt.sum().order(ascending=False)\n",
      "t4c_top20_contr_occupation = grouped_t4c_top20_contr_occupation[0:20]\n",
      "print \"\\nTop-20 Occupations with most contributors are:\\n\", t4c_top20_contr_occupation\n",
      "\n",
      "#Task 4d: Print the top-10 Employers that contributed overall (to both BO and MR)\n",
      "grouped_t4d_top10_contr_employer_all = fec.groupby('contbr_employer').contb_receipt_amt.sum().order(ascending=False)\n",
      "t4d_top10_contr_employer_all = grouped_t4d_top10_contr_employer_all[0:10]\n",
      "print \"\\nTop-10 Employers with most contributors are:\\n\", t4d_top10_contr_employer_all"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Number of contributions to Obama in each state is:\n",
        "contbr_st\n",
        "AA              40\n",
        "AB               1\n",
        "AE             122\n",
        "AK             597\n",
        "AL            1194\n",
        "AP              53\n",
        "AR             630\n",
        "AS               5\n",
        "AZ            3200\n",
        "CA           32475\n",
        "CO            3982\n",
        "CT            3169\n",
        "DC            4342\n",
        "DE             608\n",
        "FL            9857\n",
        "...\n",
        "QU              1\n",
        "RI            713\n",
        "SC           1328\n",
        "SD            219\n",
        "TN           2212\n",
        "TX           9932\n",
        "UT            841\n",
        "VA           7084\n",
        "VI            117\n",
        "VT           1498\n",
        "WA           6866\n",
        "WI           2523\n",
        "WV            418\n",
        "WY            318\n",
        "ZZ              6\n",
        "Name: contbr_nm, Length: 64, dtype: int64\n",
        "\n",
        "Top-10 states with most contributors to Obama are:\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "contbr_st\n",
        "CA           32475\n",
        "NY           17548\n",
        "IL           10726\n",
        "TX            9932\n",
        "FL            9857\n",
        "MA            8743\n",
        "MD            7551\n",
        "VA            7084\n",
        "WA            6866\n",
        "PA            6445\n",
        "Name: contbr_nm, dtype: int64\n",
        "\n",
        "Top-20 Occupations with most contributors are:\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "contbr_occupation\n",
        "RETIRED                                   36813589.97\n",
        "ATTORNEY                                  16506701.79\n",
        "HOMEMAKER                                 12396322.02\n",
        "INFORMATION REQUESTED PER BEST EFFORTS    11396894.84\n",
        "PHYSICIAN                                  5103148.90\n",
        "INFORMATION REQUESTED                      4866973.96\n",
        "PRESIDENT                                  4369754.84\n",
        "CONSULTANT                                 3884806.72\n",
        "EXECUTIVE                                  3656108.08\n",
        "LAWYER                                     3168184.07\n",
        "CEO                                        2429195.71\n",
        "INVESTOR                                   2421728.12\n",
        "PROFESSOR                                  2326433.20\n",
        "C.E.O.                                     1970076.11\n",
        "OWNER                                      1876753.60\n",
        "SELF-EMPLOYED                              1764931.84\n",
        "NOT EMPLOYED                               1709188.20\n",
        "REAL ESTATE                                1583703.09\n",
        "FINANCE                                    1439623.65\n",
        "TEACHER                                    1392307.54\n",
        "Name: contb_receipt_amt, dtype: float64\n",
        "\n",
        "Top-10 Employers with most contributors are:\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "contbr_employer\n",
        "RETIRED                                   34200584.56\n",
        "SELF-EMPLOYED                             24490846.94\n",
        "INFORMATION REQUESTED PER BEST EFFORTS    12059527.24\n",
        "HOMEMAKER                                 10752604.76\n",
        "NOT EMPLOYED                               8588808.70\n",
        "INFORMATION REQUESTED                      5053480.37\n",
        "SELF                                       1079931.20\n",
        "STUDENT                                     815322.39\n",
        "SELF EMPLOYED                               470144.24\n",
        "MORGAN STANLEY                              322614.10\n",
        "Name: contb_receipt_amt, dtype: float64\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Task 5: Basic Visualization\n",
      "-----------------------------\n",
      "\n",
      "Let us take a break from analytics to do some Visualization. Notice that the visualization tasks are NOT ordered based on their complexity. So some of them might be more challenging than others."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Task 5a: Draw a \"horizontal\" bar chart with one bar each for Obama and Romney with the value corresponding to total amount they raised.\n",
      "# Remember to make the bar chart into a horizontal bar chart\n",
      "#########################begin code for Task 5a\n",
      "#########################end code for Task 5a\n",
      "\n",
      "\n",
      "#Task 5b: Draw the \"horizontal\" bar chart of total NUMBER of contributions made per Candidate.\n",
      "# ie Candidate name vs number of contributions for that candidate\n",
      "#########################begin code for Task 5b\n",
      "#########################end code for Task 5b\n",
      "\n",
      "#Task 5c: Draw the \"horizontal\" bar chart of average contributions made per Candidate.\n",
      "# ie Candidate Name vs avg contribution\n",
      "#########################begin code for Task 5c\n",
      "#########################end code for Task 5c\n",
      "\n",
      "\n",
      "#Task 5d: Draw a \"horizontal\" bar chart that lists the top-10 states based on the TOTAL contribution they made to both candidates\n",
      "# For each state, draw two lines - one in blue for Obama and one in red for Romney\n",
      "# Display the proportion of the total contribution that came from the state.\n",
      "# For eg, if Obama made 1 billion and TX gave 100 million of it, the proportion is 10% \n",
      "# Remember to make the bar chart into a horizontal bar chart\n",
      "#########################begin code for Task 5d\n",
      "#########################end code for Task 5d\n",
      "\n",
      "\n",
      "#Task 5e: Now repeat the same process based on Occupation (again top-10)\n",
      "#########################begin code for Task 5e\n",
      "#########################end code for Task 5e\n",
      "\n",
      "\n",
      "#Task 5f: Now repeat the same process based on Employers (again top-10)\n",
      "#########################begin code for Task 5f\n",
      "#########################end code for Task 5f\n",
      "\n",
      "\n",
      "#Task 5g: Draw the histogram of total NUMBER of contributions made per each state.\n",
      "# X-axis : state, Y-axis : number of contribution from that state\n",
      "#########################begin code for Task 5g\n",
      "#########################end code for Task 5g\n",
      "\n",
      "\n",
      "#Task 5h: Draw a histogram of actual contributions made for Obama. Set bin size as 50\n",
      "#X-axis: contribution amount bin, Y-axis: frequency\n",
      "#########################begin code for Task 5h\n",
      "#########################end code for Task 5h\n",
      "\n",
      "\n",
      "#Task 5i: Draw a histogram of actual contributions made for Romney. Set bin size as 50\n",
      "#X-axis: contribution amount bin, Y-axis: frequency\n",
      "#########################begin code for Task 5i\n",
      "#########################end code for Task 5i\n",
      "\n",
      "\n",
      "#Harder\n",
      "#Task 5j: Draw a line chart showing how the campaign contribution of Obama and Romney varied every quarter\n",
      "#Use blue for Obama and red for Romney\n",
      "#This is a bit tricky because, you have to convert contribution date to quarter.\n",
      "#You can either do it on your own or see if Pandas has some function\n",
      "#########################begin code for Task 5j\n",
      "#########################end code for Task 5j\n",
      "\n",
      "\n",
      "#Harder\n",
      "#Task 5k: Draw a line chart showing the CUMULATIVE campaign contribution of Obama and Romney varied every quarter\n",
      "# In other words, if Obama made X, Y, Z in first, second and third quarters\n",
      "#  then plot X for first quarter, X+Y for second quarter and X+Y+Z for third quarter.\n",
      "#Use blue for Obama and red for Romney\n",
      "#This is a bit tricky because, you have to convert contribution date to quarter.\n",
      "#You can either do it on your own or see if Pandas has some function\n",
      "#########################begin code for Task 5k\n",
      "#########################end code for Task 5k\n",
      "\n",
      "#Tasks 5l and 5m\n",
      "#Repeat 5j and 5k but do it for NUMBER of contributors\n",
      "#In other words, 5l plots the number of contributors for Obama and Romney, quarter over quarter\n",
      "#5m plots cumulative number of contributors quarter over quarter.\n",
      "\n",
      "#########################begin code for Task 5l\n",
      "#########################end code for Task 5l\n",
      "\n",
      "\n",
      "#########################begin code for Task 5m\n",
      "#########################end code for Task 5m\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 69
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Task 6: Discretization\n",
      "-----------------------\n",
      "\n",
      "Recall the IMDB dataset that we discussed in the class where we were able to draw lot more interesting plots. The key challenge here is that most of the attributes have too many values (ie. the domain cardinality is very large). There are few ways to work around this issue. We already did something above (ie focussing only top-k). The other option is **discretization** where we create buckets and put contributions based on the buckets. Discretization in Pandas is acheived  by cut function."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#The following set of tasks are a bit tricky: \n",
      "#    you need to use multiple commands to achieve. Specifically look at cut, groupby and unstack\n",
      "\n",
      "#Task 6a: Discretize the contributions of Obama and Romney based on the bins given below.\n",
      "# For example, if Obama got contributions such as 2, 6, 16, 18, 120, then he has \n",
      "#  0 contribution in (0,1], 2 contributions in (1,10], 2 contributions in (10, 100] and 1 contribution in (100, 1000]\n",
      "bins = np.array([0, 1, 10, 100, 1000, 10000, 100000, 1000000, 10000000])\n",
      "labels = None # set the variable labels to the output of pd.cut\n",
      "grouped = None #Group the data based on labels and candidate names\n",
      "#Replace None below in the print statement with grouped.size().unstack(0) . \n",
      "#    If your code for labels and grouped is correct, this should print number of people in each bin per candidate\n",
      "print None \n",
      "\n",
      "#Task 6b: In Task 6a, we calculated the COUNT (i.e. the number of contributors in each bin)\n",
      "# This by itself is not sufficient.\n",
      "# In this task, let us compute the TOTAL AMOUNT in each bucket.\n",
      "# Specifically, compute for each candidate the total amount of contributions that each got in each bucket.\n",
      "# Continuing the example above, Obama's total contribution in each bucket is\n",
      "#  0 in (0,1], 8 in (1,10], 34 in (10, 100] and 120 in (100, 1000]\n",
      "#This could be done in 1 line from the variable grouped above\n",
      "t6b_bucket_sums = None\n",
      "print t6b_bucket_sums\n",
      "\n",
      "\n",
      "#Task 6c: Even this does not fully specify the disparity in the funding scenario.\n",
      "# To see this let us now compute the PROPORTION of total contribution in each bucket\n",
      "# This is called normalization and is a common operation. \n",
      "# Typically normalization is done over the same candidate.\n",
      "# But for the point I am trying to make, let us normalize within bucket.\n",
      "# For example, if obama made X in a bucket and Y in another bucket, \n",
      "#    then normalization will give X/(X+Y) for Obama and Y/(X+Y) for Romney\n",
      "# This is also quite easy in Pandas and can be done in 1 line\n",
      "# The key idea is to realize that sum function takes an argument called axis\n",
      "#  that does different things for axis=0 and axis=1 (figure out which does what)\n",
      "t6c_normed_bucket_sums = None\n",
      "print t6c_normed_bucket_sums\n",
      "\n",
      "#Once you have done this , uncomment the following line to a horizontal stacked bar chart\n",
      "#t6c_normed_bucket_sums[:-2].plot(kind='barh', stacked=True)\n",
      "\n",
      "\n",
      "#Task 6d: Let us go back and try to do the other analysis\n",
      "# Let us now try to see what PROPORTION of each candidate's amount came from each bucket.\n",
      "#  This is the classical case of normalization.\n",
      "#  Continuing the example above, Obama has made 0+8+34+120=162\n",
      "#  We can normalize it by diving each bucket by the total\n",
      "#   For example, 0/162, 8/162, 34/162 and 120/162.\n",
      "#  If you finished t6c, then this just a matter of playing with axis values.\n",
      "t6d_normed_bucket_sums = None\n",
      "print t6d_normed_bucket_sums\n",
      "\n",
      "\n",
      "#Once you have done this , uncomment the following line to a horizontal stacked bar chart\n",
      "#t6d_normed_bucket_sums.plot(kind='barh', stacked=True)\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "None\n",
        "None\n",
        "None\n",
        "None\n"
       ]
      }
     ],
     "prompt_number": 70
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Task 7: Big Money in Politics\n",
      "------------------------------\n",
      "\n",
      "Using the tool of discretization, we were able to analyze how much people contributed in each bucket. While it showed some sense of imbalance, it did not show it clearly. Recently, the concept of income inequality is becoming a rallying cry among liberals. Let us see if there is any inequality in the contributions (spoiler alert: yes!). We can see if we find anything interesting, not by analyzing, total amount but by the amount contributed by top-X%.\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Task 7a: Write two functions: one for Obama and one for Romney that does the following:\n",
      "# Given a value of N (N can be 1, 2, 5, 10 etc), sort the contributions made to the candidate in decreasing order\n",
      "#  Then find how much contribution the top-N% made\n",
      "#  Then compute the fraction to the overall campaign collection\n",
      "# For example, if Obama collected 1 billion dollars and the top-1% gave 100 million then their contribution is 10%\n",
      "\n",
      "def t7a_contributions_by_top_N_pct_obama(N):\n",
      "    return None\n",
      "\n",
      "def t7a_contributions_by_top_N_pct_romney(N):\n",
      "    return None\n",
      "\n",
      "\n",
      "for N in [1, 2, 5, 10, 20]:\n",
      "    print \"N=%s, Obama proportion=%s and Romney proportion = %s\" % (N, t7a_contributions_by_top_N_pct_obama(N), t7a_contributions_by_top_N_pct_romney(N))\n",
      "\n",
      "    \n",
      "\n",
      "#Task 7b: Now let us see who these people in 1% are\n",
      "# Compute the top-1% based on total contribution (ie. money they gave to Obama + Romney)\n",
      "# Now let us see some information about them.\n",
      "# For each of the folks in 1%, print the following:\n",
      "#    name, state, occupation, employer, total amount they gave to Obama, total amount they gave to Romney\n",
      "t7b_1pcters = None\n",
      "print t7b_1pcters"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "N=1, Obama proportion=None and Romney proportion = None\n",
        "N=2, Obama proportion=None and Romney proportion = None\n",
        "N=5, Obama proportion=None and Romney proportion = None\n",
        "N=10, Obama proportion=None and Romney proportion = None\n",
        "N=20, Obama proportion=None and Romney proportion = None\n",
        "None\n"
       ]
      }
     ],
     "prompt_number": 71
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Task 8: Political Polarization in USA\n",
      "------------------------------------------\n",
      "\n",
      "It has been argued that political polarization has dramatically increased in USA in the last 10-15 years. While there are many ways to analyze the dataset for polarization, let us use some simple measures."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Task 8a: One way to measure polarity is to see how different some distribution are. \n",
      "# For both Obama and Romney, print the top-10 states based on their total AMOUNT of contribution\n",
      "# Do you see lot of common states? \n",
      "t8a_top_10_states_obama = None\n",
      "t8a_top_10_states_romney = None\n",
      "\n",
      "print t8a_top_10_states_obama\n",
      "print t8a_top_10_states_romney\n",
      "\n",
      "#Task 8b: For both Obama and Romney, print the top-10 occupation based on their total AMOUNT of contribution\n",
      "# Do you see lot of common occupation? \n",
      "t8b_top_10_occu_obama = None\n",
      "t8b_top_10_occu_romney = None\n",
      "\n",
      "print t8b_top_10_occu_obama\n",
      "print t8b_top_10_occu_romney\n",
      "\n",
      "\n",
      "\n",
      "#Task 8c: For both Obama and Romney, print the top-10 employers based on their total AMOUNT of contribution\n",
      "# Do you see lot of common employers? \n",
      "t8c_top_10_emp_obama = None\n",
      "t8c_top_10_emp_romney = None\n",
      "\n",
      "print t8c_top_10_emp_obama\n",
      "print t8c_top_10_emp_romney\n",
      "\n",
      "\n",
      "#Harder\n",
      "#Task 8d: Here is another way to compute polarization\n",
      "# Find the top-1000 contributors based on their TOTAL contribution (to both Obama and Romney)\n",
      "# For each of the top-1000 folks count the number of people who donated to both, to Obama only and to Romney only\n",
      "t8d_top_1000_both = None\n",
      "t8d_top_1000_BO_only = None\n",
      "t8d_top_1000_MR_only = None\n",
      "\n",
      "print t8d_top_1000_both, t8d_top_1000_BO_only, t8d_top_1000_MR_only\n",
      "\n",
      "#Harder:\n",
      "#Task 8e: Here is yet another way\n",
      "# For each state, compute what fraction of amount went to Obama and what fraction went to Romney\n",
      "# If there is no polarization, then both will get more or less equal amount. \n",
      "# If there is polarization, then the amount will skewed.\n",
      "# Let us use a very crude measure to compute polarization\n",
      "#    If X is what Obama got from a state and Y is what Romney got, compute the value of max(X,Y)/ (min(X,Y) + 1)\n",
      "# For each state compute this value and sort the results in a descending order. \n",
      "# Do you see any pattern?\n",
      "\n",
      "t8e_state_contr_ranked_by_polarity = None\n",
      "print t8e_state_contr_ranked_by_polarity\n",
      "\n",
      "\n",
      "#Harder:\n",
      "#Task 8f: Repeat the above analysis for occupation\n",
      "# However, instead of taking all occupations, let us only take the top-50 occupations based on TOTAL contributions made\n",
      "# For each occupation compute this value and sort the results in a descending order and displ\n",
      "# Do you see any pattern?\n",
      "\n",
      "t8f_occu_contr_ranked_by_polarity = None\n",
      "print t8f_occu_contr_ranked_by_polarity\n",
      "\n",
      "\n",
      "#Harder:\n",
      "#Task 8g: A known variable of polarization is based on where a person lives.\n",
      "# At the risk of too much generalization, liberals dominate cities while conservations dominate rural areas\n",
      "# Let us see if this holds in Texas.\n",
      "# Texas is a known solid red (i.e. conservative) state.\n",
      "# For each state in Texas, compute the polarity and order them by polarity.\n",
      "# Do you see any pattern?\n",
      "t8g_tx_city_contr_ranked_by_polarity = None\n",
      "print t8g_tx_city_contr_ranked_by_polarity\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "None\n",
        "None\n",
        "None\n",
        "None\n",
        "None\n",
        "None\n",
        "None None None\n",
        "None\n",
        "None\n",
        "None\n"
       ]
      }
     ],
     "prompt_number": 72
    }
   ],
   "metadata": {}
  }
 ]
}